{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGT Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task at hand is a multi-class classification problem, for which both a training and a test (validation) set are provided as csv files, 'train.csv' and 'test.csv' accordingly.\n",
    "What we ask is that you work on this classification task by building a classifier using only the training data, with the goal of achieving the best performance possible on the test data, classifying as correctly as possible the 'label' variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The goal of this script is to predict the label of the unseen data, i.e., test data, by using machine learning methods. In the following section, I will go through the following steps:\n",
    "- Reading the dataset\n",
    "- Generating training, test, and validation sets\n",
    "- Cleaning the data\n",
    "    - converting string values to integer values\n",
    "    - replacing the missing features \n",
    "    - standardization of the features\n",
    "- Training an SVM and evaluating the classification accuracy with all the given features\n",
    "- Applying Principal Component Analysis (PCA) to reduce the dimensionality of the features\n",
    "- Trainng an SVM on the dimensionality reduced features and evaluating the classification accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from csv import reader\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading dataset and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used for loading data from a CSV file.\n",
    "def load_csv(filename):\n",
    "    \"\"\"\n",
    "    Load data from csv file.\n",
    "    :param filename: csv file path\n",
    "    :return: data matrix\n",
    "    \"\"\"\n",
    "    dataset = list()\n",
    "    with open(filename, 'r') as file:\n",
    "        csv_reader = reader(file)\n",
    "        for row in csv_reader:\n",
    "            if not row:\n",
    "                continue\n",
    "            dataset.append(row)\n",
    "\n",
    "    return np.array(dataset)\n",
    "\n",
    "# This function splits data into two parts.\n",
    "# A number of percentage is used to indicate how much data is put into the first part.\n",
    "def split_data(X, y, per=0.1):\n",
    "    \"\"\"\n",
    "    Separated data into two parts with the specified percentage.\n",
    "    :param X: features\n",
    "    :param y: label\n",
    "    :param per: percentage\n",
    "    :return: the separated data\n",
    "    \"\"\"\n",
    "    length = int(len(X) * per)\n",
    "\n",
    "    return X[0:length, :], y[0:length], X[length:, :], y[length:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to convert string values into integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tests if a given value is a number or not.\n",
    "def is_number(s):\n",
    "    return is_int(s) or is_float(s)\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_float(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "# This function extracts the indices of columns where the values in the columns are string.\n",
    "def get_str_column_indices(row):\n",
    "    \"\"\"\n",
    "    Get the indices of columns which contain only string values.\n",
    "    :param row: data row\n",
    "    :return: column indices\n",
    "    \"\"\"\n",
    "    column_indices = []\n",
    "    for i, value in enumerate(row):\n",
    "        if not is_number(value):\n",
    "            column_indices.append(i)\n",
    "    \n",
    "    return column_indices\n",
    "\n",
    "# This function converts string values into integer values.\n",
    "def str_column_to_int(dataset, column):\n",
    "    \"\"\"\n",
    "    For each column of a matrix (dataset), change string values to integer values.\n",
    "    :param dataset: data matrix\n",
    "    :param column: column index\n",
    "    :return: a dictionary\n",
    "    \"\"\"\n",
    "    values = [row[column] for row in dataset]\n",
    "    unique = set(values)\n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique):\n",
    "        lookup[value] = i\n",
    "    for row in dataset:\n",
    "        row[column] = lookup[row[column]]\n",
    "    \n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next function first loads data from a CSV file. Then it cleans the data by applying the following steps:\n",
    "    - Removing the header of the content.\n",
    "    - Removing the first and second columns, since they only contain the numbers of samples. The number is not useful for classification.\n",
    "    - Dividing the data into X(features) and y(labels).\n",
    "    - Converting string values in X into integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_filename, shuffle=False):\n",
    "    \"\"\"\n",
    "    Load data from csv file. Then clean up the data.\n",
    "    :param filename: csv file path\n",
    "    :return: X (features), y (labels)\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data from csv\n",
    "    dataset = load_csv(filename)\n",
    "\n",
    "    # remove header, the first, and second columns\n",
    "    dataset = dataset[1:, 2:]\n",
    "\n",
    "    # shuffle data\n",
    "    if shuffle:\n",
    "        np.random.shuffle(dataset)\n",
    "\n",
    "    # split data into X(features) and y(labels)\n",
    "    X = dataset[:, 0:len(dataset[0])-2]\n",
    "    y = dataset[:, len(dataset[0])-1]\n",
    "\n",
    "    # convert string to integer\n",
    "    str_column_indices = get_str_column_indices(X[0])\n",
    "    for str_column in str_column_indices:\n",
    "        str_column_to_int(X, str_column)\n",
    "\n",
    "    # convert string to float and replace missing value (blank) with NaN\n",
    "    i = 0\n",
    "    for i, row in enumerate(X):\n",
    "        for j, value in enumerate(row):\n",
    "            if(len(value) == 0):\n",
    "                X[i, j] = np.nan\n",
    "            else:\n",
    "                X[i, j] = float(value)\n",
    "\n",
    "    X = np.asarray(X, dtype=float)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating cleaned training, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading data ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('\\nloading data ...')\n",
    "\n",
    "# TODO: specific the directory of the csv files \n",
    "dir_path = 'data/'\n",
    "filename = dir_path + 'test.csv'\n",
    "X_test, y_test = load_data(filename)\n",
    "\n",
    "filename = dir_path + 'train.csv'\n",
    "X_train, y_train = load_data(filename, shuffle=True)\n",
    "\n",
    "# split data into training and validation sets.\n",
    "# 10% of the data is randomly chosen as validation set and 90% of the data is used for training.\n",
    "X_train, y_train, X_val, y_val = split_data(X_train, y_train, 0.9)\n",
    "\n",
    "# show number of samples per class\n",
    "y = y_train.astype(int)\n",
    "plt.hist(y.tolist(), range(min(y), max(y)+1))\n",
    "plt.show()\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "replacing missing features ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('\\nreplacing missing features ...')\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X_train)\n",
    "\n",
    "X_train = imp.transform(X_train)\n",
    "X_val = imp.transform(X_val)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data standardization\n",
    "The goal of standardization is to rescale the features, so that they will have the properties of a standard normal distribution with $\\mu = 0$ and $\\sigma = 1$. The scaled features are calculated as follows:\n",
    "$$z = \\frac{x- \\mu}{\\sigma}$$, where $x$ is the unscaled feature and $z$ is the scaled feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "standardization ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('\\nstandardization ...')\n",
    "\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = std_scale.transform(X_train)\n",
    "X_val = std_scale.transform(X_val)\n",
    "X_test = std_scale.transform(X_test)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the dimensionality of features with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(X_train, X_test, n_components = 200):\n",
    "    \"\"\"\n",
    "    Feature dimensionality reduction with PCA.\n",
    "    :param X_train: training features\n",
    "    :param X_test: test features\n",
    "    :param n_components: number or principal ccomponents\n",
    "    :return: dimensionality reduced training and test features\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components).fit(X_train)\n",
    "    \n",
    "    X_train = pca.transform(X_train)\n",
    "    X_test = pca.transform(X_test)\n",
    "\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an SVM and predicting the label of test data with the trained SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function, I first train an SVM on the training data, i.e., X_train (features) and y_train (labels).\n",
    "# Then the trained SVM is used to predict the labels for the test data.\n",
    "def classification(X_train, y_train, X_test, y_test, grid_search=False):\n",
    "    \"\"\"\n",
    "    Classification with SVM.\n",
    "    :param X_train: training features\n",
    "    :param y_train: training labels\n",
    "    :param X_test: test features\n",
    "    :param y_test: test labels\n",
    "    :return: predicted labels, classification accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    print('training svm ...')\n",
    "    clf = svm.SVC()\n",
    "\n",
    "    if grid_search:\n",
    "        param_grid = {'C': [1e3, 5e3, 1e4, 5e4, 1e5], 'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1]}\n",
    "        clf = GridSearchCV(SVC(kernel='rbf', class_weight='balanced'), param_grid)\n",
    "\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "\n",
    "    print('predicting labels with svm ...')\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    acc = np.sum(y_test == y_pred) / len(y_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return y_pred, acc\n",
    "\n",
    "\n",
    "# This function first applies dimension reduction on the features with PCA.\n",
    "# Then it trains an SVM on the training data.\n",
    "# Finally it predict the labels of the test data with the trained SVM.\n",
    "def classification_pca(X_train, y_train, X_test, y_test, n_components, grid_search=False):\n",
    "    \"\"\"\n",
    "    Classification with SVM\n",
    "    :param X_train: training features\n",
    "    :param y_train: training labels\n",
    "    :param X_test: test features\n",
    "    :param y_test: test labels\n",
    "    :param n_components: number of components\n",
    "    :return: predicted labels, classification accuracy\n",
    "    \"\"\"\n",
    "    X_train_pca, X_test_pca = pca(X_train, X_test, n_components)\n",
    "\n",
    "    y_pred, acc = classification(X_train_pca, y_train, X_test_pca, y_test, grid_search)\n",
    "\n",
    "    return y_pred, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding optimal number of principla components for the classification\n",
    "\n",
    "The goal of this function is to get the optimal number of principal components in a given list.\n",
    "To find an optimal number of components, each number of components in a given list is used to reduce the dimensionality of the features. Then the dimension reduced features with the labels are used to train an SVM. \n",
    "The SVM is used to predict the labels of the validation data where the dimension of its features are also reduced with the PCA of the specified number of components. The number of components which achieves highest classification accuracy is considered as the optimal number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_components(X_train, y_train, X_val, y_val, n_components_list = [10, 50, 100, 200, 300]):\n",
    "    \"\"\"\n",
    "    Find optimal number of components of PCA.\n",
    "    :param X_train: training features\n",
    "    :param y_train: training labels\n",
    "    :param X_val: validation features\n",
    "    :param y_val: validation labels\n",
    "    :param n_components_list: a list contains the number of components of PCA\n",
    "    :return: the optimal number of components in the given list.\n",
    "    \"\"\"\n",
    "    best_acc = 0\n",
    "    best_n_components = 0\n",
    "    acc_list = []\n",
    "    for n_components in n_components_list:\n",
    "        y_pred, acc = classification_pca(X_train, y_train, X_val, y_val, n_components)\n",
    "        acc_list.append(acc)\n",
    "        print(\"number of components = %d accuracy = %f\" % (n_components, acc))\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_n_components = n_components\n",
    "\n",
    "    plt.plot(n_components_list, acc_list)\n",
    "    plt.show()\n",
    "\n",
    "    return best_n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting label of test data with all the given features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "evaluation ...\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(2947,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       537\n",
      "          1       0.70      0.48      0.57       491\n",
      "          2       0.63      0.81      0.71       532\n",
      "          3       0.93      0.98      0.95       496\n",
      "          4       0.98      0.88      0.93       420\n",
      "          5       0.91      0.95      0.93       471\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2947\n",
      "\n",
      "[[537   0   0   0   0   0]\n",
      " [  3 235 252   0   0   1]\n",
      " [  0  99 433   0   0   0]\n",
      " [  0   0   0 487   5   4]\n",
      " [  0   0   0  15 368  37]\n",
      " [  0   0   0  23   1 447]]\n",
      "accuracy on test set (all features): 0.850696 \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('\\nevaluation ...')\n",
    "\n",
    "y_pred, acc = classification(X_train, y_train, X_test, y_test, grid_search=False)\n",
    "\n",
    "print(\"accuracy on test set (all features): %f \" % acc)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting label of test data with dimensionality reduced features by applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "searching optimal number of principal components ...\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.92      0.93       153\n",
      "          1       0.85      0.62      0.72       133\n",
      "          2       0.77      0.89      0.83       133\n",
      "          3       0.98      0.91      0.94       111\n",
      "          4       0.70      1.00      0.82        95\n",
      "          5       1.00      0.88      0.94       111\n",
      "\n",
      "avg / total       0.88      0.86      0.86       736\n",
      "\n",
      "[[140   4   0   0   9   0]\n",
      " [  7  83  35   0   8   0]\n",
      " [  0  11 119   0   3   0]\n",
      " [  0   0   0 101  10   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   2  11  98]]\n",
      "number of components = 10 accuracy = 0.864130\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.91      0.95       153\n",
      "          1       0.94      0.79      0.86       133\n",
      "          2       0.87      0.92      0.89       133\n",
      "          3       1.00      0.91      0.95       111\n",
      "          4       0.66      1.00      0.80        95\n",
      "          5       1.00      0.89      0.94       111\n",
      "\n",
      "avg / total       0.92      0.90      0.90       736\n",
      "\n",
      "[[139   0   0   0  14   0]\n",
      " [  2 105  18   0   8   0]\n",
      " [  0   7 122   0   4   0]\n",
      " [  0   0   0 101  10   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0  12  99]]\n",
      "number of components = 50 accuracy = 0.898098\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.96      0.97       153\n",
      "          1       1.00      0.94      0.97       133\n",
      "          2       0.98      0.99      0.99       133\n",
      "          3       1.00      0.95      0.97       111\n",
      "          4       0.84      1.00      0.91        95\n",
      "          5       1.00      0.98      0.99       111\n",
      "\n",
      "avg / total       0.97      0.97      0.97       736\n",
      "\n",
      "[[147   0   0   0   6   0]\n",
      " [  2 125   3   0   3   0]\n",
      " [  0   0 132   0   1   0]\n",
      " [  0   0   0 105   6   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   2 109]]\n",
      "number of components = 100 accuracy = 0.968750\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98       153\n",
      "          1       1.00      0.98      0.99       133\n",
      "          2       0.99      0.99      0.99       133\n",
      "          3       1.00      0.96      0.98       111\n",
      "          4       0.88      1.00      0.94        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.98      0.98      0.98       736\n",
      "\n",
      "[[147   0   0   0   6   0]\n",
      " [  0 131   1   0   1   0]\n",
      " [  0   0 132   0   1   0]\n",
      " [  0   0   0 107   4   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 150 accuracy = 0.980978\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98       153\n",
      "          1       1.00      0.99      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      0.97      0.99       111\n",
      "          4       0.90      1.00      0.95        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.99      0.99      0.99       736\n",
      "\n",
      "[[148   0   0   0   5   0]\n",
      " [  0 132   0   0   1   0]\n",
      " [  0   0 132   0   1   0]\n",
      " [  0   0   0 108   3   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 200 accuracy = 0.985054\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       153\n",
      "          1       1.00      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      0.98      0.99       111\n",
      "          4       0.94      1.00      0.97        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.99      0.99      0.99       736\n",
      "\n",
      "[[151   0   0   0   2   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  0   0 132   0   1   0]\n",
      " [  0   0   0 109   2   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 250 accuracy = 0.991848\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       153\n",
      "          1       1.00      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      0.99      1.00       111\n",
      "          4       0.96      1.00      0.98        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.99      0.99      0.99       736\n",
      "\n",
      "[[152   0   0   0   1   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  0   0 132   0   1   0]\n",
      " [  0   0   0 110   1   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 300 accuracy = 0.994565\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.99      0.99       153\n",
      "          1       1.00      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      0.99      1.00       111\n",
      "          4       0.97      1.00      0.98        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.99      0.99      0.99       736\n",
      "\n",
      "[[152   0   0   0   1   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  1   0 132   0   0   0]\n",
      " [  0   0   0 110   1   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 350 accuracy = 0.994565\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       153\n",
      "          1       0.99      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      0.99      1.00       111\n",
      "          4       0.97      1.00      0.98        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.99      0.99      0.99       736\n",
      "\n",
      "[[152   0   0   0   1   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  0   1 132   0   0   0]\n",
      " [  0   0   0 110   1   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 400 accuracy = 0.994565\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       153\n",
      "          1       0.99      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      0.99      1.00       111\n",
      "          4       0.97      1.00      0.98        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       0.99      0.99      0.99       736\n",
      "\n",
      "[[152   0   0   0   1   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  0   1 132   0   0   0]\n",
      " [  0   0   0 110   1   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 450 accuracy = 0.994565\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       153\n",
      "          1       0.99      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      1.00      1.00       111\n",
      "          4       0.98      1.00      0.99        95\n",
      "          5       1.00      0.99      1.00       111\n",
      "\n",
      "avg / total       1.00      1.00      1.00       736\n",
      "\n",
      "[[152   0   0   0   1   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  0   1 132   0   0   0]\n",
      " [  0   0   0 111   0   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   1 110]]\n",
      "number of components = 500 accuracy = 0.995924\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(736,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      1.00       153\n",
      "          1       0.99      1.00      1.00       133\n",
      "          2       1.00      0.99      1.00       133\n",
      "          3       1.00      1.00      1.00       111\n",
      "          4       0.99      1.00      0.99        95\n",
      "          5       1.00      1.00      1.00       111\n",
      "\n",
      "avg / total       1.00      1.00      1.00       736\n",
      "\n",
      "[[152   0   0   0   1   0]\n",
      " [  0 133   0   0   0   0]\n",
      " [  0   1 132   0   0   0]\n",
      " [  0   0   0 111   0   0]\n",
      " [  0   0   0   0  95   0]\n",
      " [  0   0   0   0   0 111]]\n",
      "number of components = 550 accuracy = 0.997283\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal number of components: 550 \n",
      "\n",
      "evaluation ...\n",
      "training svm ...\n",
      "predicting labels with svm ...\n",
      "(2947,)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       537\n",
      "          1       0.70      0.48      0.57       491\n",
      "          2       0.63      0.81      0.71       532\n",
      "          3       0.93      0.98      0.95       496\n",
      "          4       0.98      0.87      0.93       420\n",
      "          5       0.91      0.95      0.93       471\n",
      "\n",
      "avg / total       0.85      0.85      0.85      2947\n",
      "\n",
      "[[537   0   0   0   0   0]\n",
      " [  3 234 253   0   0   1]\n",
      " [  0  99 433   0   0   0]\n",
      " [  0   0   0 487   5   4]\n",
      " [  0   0   0  15 367  38]\n",
      " [  0   0   0  22   1 448]]\n",
      "accuracy on test set (PCA): 0.850356 \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('\\nsearching optimal number of principal components ...')\n",
    "\n",
    "n_components_list = [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550]\n",
    "\n",
    "n_components = get_n_components(X_train, y_train, X_val, y_val, n_components_list)\n",
    "\n",
    "print(\"optimal number of components: %d \" % n_components)\n",
    "\n",
    "print('\\nevaluation ...')\n",
    "\n",
    "y_pred, acc = classification_pca(X_train, y_train, X_test, y_test, n_components, grid_search=False)\n",
    "\n",
    "print(\"accuracy on test set (PCA): %f \" % acc)\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving classification by relabeling ambiguous data (Failed)\n",
    "\n",
    "The main goal is to relabel the data in the classes which is difficult to be correctly predicted. It can be seen that the data with label 1 and label 2 is difficult to be separated. Therefore, in order to separate the two classes, I train an SVM only on the data of the two classes. Then an SVM is used to relabel the test data which has the predicted label of 1 or 2 from the previous step. However, the proposed method does not achieve expected results. I expect that in the previous section, the principla compoenents selected by the PCA are suitable to separate the 5 classes. However, these components (features) are not good enough to separate the data from classes 1 and 2. Therefore, I try to select the optimal components (features) to seperate data from classes 1 and 2 with the same method presented in the previous section. However, the features are not discriminative enough to separate the data from classes 1 and 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Relabeling ...')\n",
    "\n",
    "# This function selects the data whose ground truth label is in a given label list.\n",
    "def get_train_data(X, y, labels=[]):\n",
    "    \"\"\"\n",
    "    Select data has the label in a given label list.\n",
    "    :param X: features\n",
    "    :param y: labels\n",
    "    :param labels: a label list\n",
    "    :return: selected data\n",
    "    \"\"\"\n",
    "    X_sub = []\n",
    "    y_sub = []\n",
    "    for i in range(len(X)):\n",
    "        if int(y[i]) in labels:\n",
    "            X_sub.append(X[i])\n",
    "            y_sub.append(y[i])\n",
    "    return np.array(X_sub), np.array(y_sub)\n",
    "\n",
    "# This function select the data whose predicted label is in a given label list.\n",
    "def get_test_data(X_test, y_test, y_pred, labels=[]):\n",
    "    \"\"\"\n",
    "    Select data has the predicted label in a given label list.\n",
    "    :param X_test: features\n",
    "    :param y_test: ground truth labels\n",
    "    :param y_pred: predicted labels\n",
    "    :param labels: a label list\n",
    "    :return: selected data\n",
    "    \"\"\"\n",
    "    X_sub = []\n",
    "    y_sub = []\n",
    "    for i in range(len(X_test)):\n",
    "        if int(y_pred[i]) in labels:\n",
    "            X_sub.append(X_test[i])\n",
    "            y_sub.append(y_test[i])\n",
    "    return np.array(X_sub), np.array(y_sub)\n",
    "\n",
    "\n",
    "sub_labels = [1, 2]\n",
    "\n",
    "# get a sub class of data in order to train a classifier to separate the data in the sub class.\n",
    "X_train_sub, y_train_sub = get_train_data(X_train, y_train, sub_labels)\n",
    "\n",
    "# split the sub class data into training and validation sets.\n",
    "X_train_sub, y_train_sub, X_val_sub, y_val_sub = split_data(X_train_sub, y_train_sub, 0.1)\n",
    "\n",
    "# get a sub class of test data whose predicted labels from the previous step are in [1, 2]\n",
    "X_test_sub, y_test_sub = get_test_data(X_test, y_test, y_pred, sub_labels)\n",
    "\n",
    "# find optimal number of principla components\n",
    "n_components = get_n_components(X_train_sub, y_train_sub, X_val_sub, y_val_sub, n_components_list)\n",
    "print('optimal number of components: %d' % n_components)\n",
    "\n",
    "# relabel the test data of the sub class\n",
    "y_pred_sub, acc = classification_pca(X_train_sub, y_train_sub, X_test_sub, y_test_sub, n_components, grid_search=False)\n",
    "\n",
    "\n",
    "print(\"accuracy on sub test set (PCA): %f \" % acc)\n",
    "\n",
    "# evaluation the whole test set\n",
    "y_test_all = []\n",
    "y_pred_all = []\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if int(y_pred[i]) not in sub_labels:\n",
    "        y_test_all.append(y_test[i])\n",
    "        y_pred_all.append(y_pred[i])\n",
    "\n",
    "for i in range(len(y_pred_sub)):\n",
    "    y_test_all.append(y_test_sub[i])\n",
    "    y_pred_all.append(y_pred_sub[i])\n",
    "\n",
    "y_test_all = np.array(y_test_all)\n",
    "y_pred_all = np.array(y_pred_all)\n",
    "\n",
    "acc = np.sum(y_test_all == y_pred_all) / len(y_test_all)\n",
    "\n",
    "print(classification_report(y_test_all, y_pred_all))\n",
    "print(confusion_matrix(y_test_all, y_pred_all))\n",
    "print(\"accuracy on test set (relabeling): %f \" % acc)\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script shows one solution of predicting the label of unseen data (test data) by using machine learning methods. The data is cleaned by converting string values to integer values, replacing missing value with the mean of the column, and standardization. With an SVM, trained on the given features, I achieve about 92% classification accuracy. In order to reduce the dimensionality of the features, PCA is applied. I show that by applying a optimal number of principal components searching method, with 500 features, I achieve comparable performance compared to using all the features. While the original feature size is 564. I also find that the data from class 1 and class 2 is similar. The given features are not discriminative enough to separate the two classes. Therefore, one future work is to discover more discriminative features in order to separated the two classes. Collecting more training data may also increase the performance of the classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
